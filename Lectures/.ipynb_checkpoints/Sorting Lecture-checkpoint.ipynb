{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting\n",
    "\n",
    "In this lecture we will focus on algorithms to sort elements. The sorting algorithms we will cover in order of descending time complexity\n",
    "\n",
    "- Insertion Sort\n",
    "- Bubble Sort\n",
    "- Shellsort\n",
    "- Heapsort\n",
    "- Merge Sort \n",
    "- Quicksort\n",
    "- Bucket Sort\n",
    "- Radix Sort\n",
    "- Count Sort\n",
    "\n",
    "\n",
    "## Insertion Sort\n",
    "One of the simpliest sorting algorithms is **insertion sort**. Insertion sort consist of N-1 **passes**. At each pass, p, we ensure that the array from 0 to p is in sorted order. Since we know that all the elements from 0 to p-1 are in sorted order we need to find out where element p belongs in the already sorted list. So in each pass p we move the element in position p left until it's in the correct position. Lets look at how this is implemented\n",
    "```python\n",
    "def insertion_sort(arr): \n",
    "    # iterate through the array\n",
    "    for i in range(1, len(arr)): \n",
    "  \n",
    "        key = arr[i] \n",
    "  \n",
    "        # Move elements of arr[0..i-1], that are \n",
    "        # greater than key, to one position ahead \n",
    "        # of their current position \n",
    "        j = i\n",
    "        while j > 0 and key < arr[j-1] : \n",
    "                arr[j] = arr[j-1] \n",
    "                j -= 1\n",
    "        arr[j] = key\n",
    "    return arr\n",
    "```\n",
    "Because of the nested loops, each of which can take N iterations, insertion sort runs in $O(N^2)$. We could easily see this is possible if the input is reversed. However if the list is already sorted then the inner while loop never runs hence giving it O(N) running time. \n",
    "Lets look at a visualization of the algorithm. \n",
    "![SegmentLocal](./files/Sorting/insertion_sort.gif \"segment\")\n",
    "\n",
    "## Bubble Sort\n",
    "Another extremely simple sorting algorithm is **bubble sort**. Bubble sort works by repeatedly stepping through the list and comparing each pair of adjacent elements and swapping them if they are in the wrong order. The algorithm continually steps through the list until no swaps are performed during a pass. During each pass bubble sort is finding the $n^{th}$ largest element and putting it into the correct place. Similar to insertion sort, the running time of this algorithm is $O(N^2)$. Hence if the list is in reversed order it will take N passes to find the smallest element in the list and put it in the correct position. Let's look at how bubble sort is implemented\n",
    "```python\n",
    "def bubble_sort(arr):\n",
    "    # There is also no do while in python\n",
    "    # so we will emulate one by initializing swapped to True\n",
    "    # and setting it to false at the beginning of the loop\n",
    "    swapped = True\n",
    "    while swapped:\n",
    "        swapped = False\n",
    "        # Perform a pass on the array \n",
    "        for i in range(len(arr)-1):\n",
    "            # If the elements are in the wrong order swap them\n",
    "            # set swapped to True as a swap has been performed\n",
    "            if arr[i] > arr[i+1]:\n",
    "                arr[i], arr[i+1] = arr[i+1], arr[i]\n",
    "                swapped = True\n",
    "    return arr\n",
    "```\n",
    "Lets look at a visualization of the algorithm in work.\n",
    "![SegmentLocal](./files/Sorting/bubble_sort.gif \"segment\")\n",
    "\n",
    "## Shell Sort\n",
    "Shell sort takes the idea of insertion sort and generalizes it. The idea is that to arrange the list of elements so that starting from anywhere every $h^{th}$ element from that position is sorted, or we can state that for every i, a\\[i\\] $\\le$ a\\[i+h\\]. The list at this point is said to be **h-sorted**. The algorithm starts by initially considering a large h and then decreases until the last phase in which h=1. The question is how to determine the sequence of h (gaps) to use when performing the sort. Let's look at an example if we chose 5,3,1 as the gaps.\n",
    "\n",
    "<img src=\"./files/Sorting/shellsort.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "Shell suggest $h_1 = N/2$, and $h_k = N/2^k$. However the problem with Shell's method is that is has a running time of $O(N^2)$ so there is no obvious case for using it over insertion or bubble sort. We can show that the upper bound using Shell's method is $O(N^2)$. Each pass with an increment of $h_k$ consists of $h_k$ insertion sorts of $N/h_k$ elements. Since insertion sort is quadratic, the total cost of a pass is $O(h_k(N/h_k)^2) = O(N^2/h_k)$. Summing over all the passes gives a total bound of $\\sum_{i=1}^{t} N^2/h_i = N^2\\sum_{i=1}^{t} 1/h_i$. Since the increments form a geometric series with a common ratio of 2, and the largest term in the series is 1, $\\sum_{i=1}^{t} 1/h_i < 2$. This we obtain a upper bound of $O(N^2)$.\n",
    "\n",
    "The problem with Shell's method is that the increments are not relatively prime, and thus smaller increments can have little effect. Hibbard sugggest a slightly different increment sequence, giving a better run time of $O(N^{3/2})$. Hibbard's method states to use increments of the form $1,3,7,...,2^k-1$. The proof of this requires using additive number theory which is beyond the scope of this class and therefore left out. \n",
    "\n",
    "There are other methods that have been applied to shell sort not just Shell's and Hibbard's. Prat showed that using $O(N^{3/2})$ applies to a wide range of increment sequences. Whereas Sedgewick has proposed multiple increment sequences that give an $O(N^{4/3})$ worst case running time using $4^i+3 * 2^i + 1$. There are several more that use theorems from number theory and combinatorics such as Cuira's method which has yet to have a proven run time. Hence why the actual running time of shell sort is unknown.\n",
    "\n",
    "Let's look at code for using Hibbards algorithm\n",
    "```python\n",
    "def shell_sort(arr):\n",
    "    # Create the increment sequence\n",
    "    gaps = []\n",
    "    for i in range(1, int(log(len(arr),2))):\n",
    "        gaps.insert(0,(2 ** i - 1))\n",
    "    \n",
    "    for gap in gaps:\n",
    "        # perform generalized insertion sort for that gap\n",
    "        for i in range(gap, len(arr)):\n",
    "            key = arr[i]\n",
    "            j = i\n",
    "            while j >= gap and key < arr[j-gap]:\n",
    "                arr[j] = arr[j-gap]\n",
    "                j -= gap\n",
    "            \n",
    "            arr[j] = key\n",
    "        \n",
    "    return arr\n",
    "```\n",
    "\n",
    "## Heap Sort\n",
    "Binary Heaps (Priority Queues) can be used to sort in O(NlogN) time. So far this algorithm gives the best running time for sorting algorithms. Recall that the basic strategy of a min heap is that the minimum element is always at the root. So the idea is that given an unsorted list to build a binary heap of N elements (this takes O(N) time). We then perform N deleteMin operations getting the elements in sorted order. Each time we perform a deleteMin we save the result in a second array to hold the sorted elements. Since the running time of deleteMin is O(logN) and we perform it N times the total running time is O(NlogN). <br>\n",
    "The only problem with this algorithm is space complexity since it uses a second array to hold the sorted items. This is not usually a problem as memory is cheap and plentiful but could become an issue if the input is substantially large. A clever way to avoid using a second array is to use the array representing the binary heap to maintain the sorted order. Since after each delete the heap decreases size by 1 the cell that was last in the heap could be used to store the removal. If we were to use this strategy on a min heap it would return an array of items sorted in reverse order. To fix this instead of using a min heap we now use a max heap. Assuming we are using a max heap here is how we would implement a heap sort inside the heap class. We will use the same stucture as the binary heap class we implemented in the heaps lecture.\n",
    "```python\n",
    "def heap_sort(arr):\n",
    "    # first build a heap\n",
    "    self.build_heap(arr)\n",
    "    \n",
    "    # perform N delete maxes and place the max element in the spot that was just removed\n",
    "    # This sorts the array that the heap is built on\n",
    "    for i in range(len(self.heap)):\n",
    "        element = self.delete_max()\n",
    "        self.heap[self.size+1] = element\n",
    "    \n",
    "    # The array maintaining the heap is now sorted\n",
    "    # We remove the first element since the indexes in a heap start at 1\n",
    "    return self.heap[1:]\n",
    "```\n",
    "\n",
    "## Merge Sort \n",
    "The fundamental operation in mergesort is merging two sorted list. Because the lists are sorted, this can be done in one pass through the two already sorted list and the output a sorted list. This merging takes two sorted lists A and B, and output list C. The way we do this is pass through both list, A and B, simultaneously. We have counters for both A and B and while passing through the list we append the smaller of the two onto list C. We do this until we have fully passed through both list. Lets look at an example of how the merge operation works.\n",
    "\n",
    "Let array A contain \\[1, 13, 24, 26\\] and B contain \\[2, 15, 27, 38\\]. Let the counters for A and B be i and j respectively. \n",
    "<img src=\"./files/Sorting/mergesort_merge.png\" width=\"500\"/>\n",
    "\n",
    "We can see that once the counter has reached the end we just append the rest of the other list onto the output list since it is already sorted. The run time of this merge is O(N) since at most N-1 comparisons need to be made, where N is the total number of elements in the two lists. \n",
    "\n",
    "The algorithm is therefore easy to describe. If N = 1, there is only element to sort. Otherwise, recursively mergsort the first half and the second half. This gives two sorted halves which can be merged together using the merging algorithm. This algorithm is a classic divide-and-conquer strategy. The problem is *divided* into smaller problems and solved recursively. The *conquering* phase consists of patching together the answers. Lets look at the example:\n",
    "<img src=\"./files/Sorting/margesort.png\" width=\"500\"/>\n",
    "\n",
    "Lets look at the code\n",
    "```python\n",
    "# First the merge of two sorted arrays\n",
    "def merge(left, right):\n",
    "    # counters for both lists\n",
    "    counter_left, counter_right = 0\n",
    "    \n",
    "    # length of the smaller of the two list\n",
    "    min_length = min(len(left, right))\n",
    "    \n",
    "    # output list to hold sorted merge\n",
    "    sorted_output = []\n",
    "    \n",
    "    # loop through both lists simultaneously\n",
    "    while counter_left < min_length and counter_right < min_length:\n",
    "        if left[counter_left] < right[counter_right]:\n",
    "            sorted_output.append(left[counter_left])\n",
    "            counter_left += 1\n",
    "        else:\n",
    "            sorted_output.append(right[counter_right])\n",
    "            counter_right += 1\n",
    "            \n",
    "    # After exhausting one list append the rest of the list to the output\n",
    "    if counter_right < len(right):\n",
    "        sorted_output.extend(right[counter_right:])\n",
    "    elif counter_left < len(left):\n",
    "        sorted_output.extend(left[counter_left:])\n",
    "        \n",
    "    return sorted_output\n",
    "\n",
    "\n",
    "# Merge sort\n",
    "def merge_sort(arr):\n",
    "    if len(arr) == 1:\n",
    "        return arr\n",
    "    else:\n",
    "        center = len(arr) // 2\n",
    "        # split the arrray in half\n",
    "        left = merge_sort(arr[:center])\n",
    "        right = merge_sort(arr[center:])\n",
    "        # return the merge of the two sorted list\n",
    "        return merge(left, right)\n",
    "```\n",
    "\n",
    "### Running Time Analysis\n",
    "The runtime of mergesort is O(NlogN) but how do we know this? Lets prove it with recurrence relations. For N = 1, the time to mergesort is constant since a single element is already sorted, we will denote this by 1. Otherwise, the time to mergesort N numbers is equal to the time to do two recursive mergesorts of size N/2, plus the time to merge (which is linear). The following equations:\n",
    "<center> T(1) = 1 <br> T(N) = 2T(N/2)+N </center>\n",
    "Lets solve the reccurrence relations as follows:\n",
    "<center>\n",
    "    T(N) = 2T(N/2)+N <br>\n",
    "         = 2(2T(N/4) + N/2) + N = 4T(N/4) + 2N <br>\n",
    "         = 4(2T(N/8) + N/4) + 2N = 8T(N/8) + 3N <br>\n",
    "                           ...<br>\n",
    "         = $2^kT(N/2^k) + kN$ \n",
    "</center>\n",
    "Using k = logN, we obtain T(N) = NT(1) + NlogN = NlogN + N which gives us the running time of O(NlogN).\n",
    "\n",
    "## Quicksort\n",
    "Like mergesort, quicksort is a divide-and-conquer recursive algorithm. The basic algorithm to sort an array S consists of the following four steps:\n",
    "1. If the number of elements in S is 0 or 1, return\n",
    "2. Pick any element v in S. This is called the **pivot**\n",
    "3. **Partition** S - {v} into two disjoint groups: $S_1 = \\{x \\in S - {v} \\mid x \\leq v\\}$, and $S_2 = \\{x \\in S - {v} \\mid x \\geq v\\}$\n",
    "4. Return quicksort($S_1$) followed by v followed by quicksort($S_2$)\n",
    "\n",
    "This shows at a high level how the algorithm works.\n",
    "<img src=\"./files/Sorting/quicksort_simple.png\" width=\"500\"/>\n",
    "\n",
    "### Picking the Pivot\n",
    "The **wrong way** of picking the pivot would be to use the first element. This is because if the partition happens to be partially sorted or in reverse order the pivot will provide a bad partition because all the elements will either go into $S_1$ or $S_2$. Doing this will result in quicksort running in $O(N^2)$ so it will be no better than a simple sorting algorithm. A **safe way** of picking the pivot would to chose randomly. \n",
    "\n",
    "#### Median-of-Three Partitioning\n",
    "A better method of choosing the pivot is to use the median. The median of the array would be the best choice of a pivot however calculating the median of an unsorted array would slow down the algorithm. Therefore we chose three elements from the array and use the median of the three as the pivot. The common and best practice choices are the first, middle, and last element in the array. So for instance if the array is \\[8,1,4,9,6,3,5,2,7,0\\], the first element is 8, middle element is 6, and last element is 0, thus the median would be 6. Using this method of obtaining a pivot eliminates the bad case for sorted input and reversed input. \n",
    "\n",
    "### Partitioning Strategy\n",
    "Now we discuss the strategy on how to partition the list into $S_1$ and $S_2$. The first step is to move the pivot element out of the way. We do this by swapping the pivot element with the last element in the array. We have two variables i and j, i starts at the beginning of the array and j starts at the next-to-last element. We will use the array \\[8,1,4,9,6,3,5,2,7,0\\] for an example. After the initial swap we are left with the following array with i and j in the correct places. \n",
    "<img src=\"./files/Sorting/quicksort_pivot_1.png\" width=\"500\"/>\n",
    "\n",
    "The idea behind the strategy is to move all the small elements to the left and all the large elements to the right. Small and large are relative to what element is picked as the pivot. So while $i \\lt j$, we move i right, skipping over elements that are smaller than the pivot, and stop when we find an element that is larger. We then move j to the left, skipping over elements that are larger than the pivot, and stop when we find an element is smaller. When both i and j have stopped, i is pointing to an element larger than the pivot and j is pointing to an element smaller than the pivot. If $i \\lt j$ then we swap both of those elements and continue. Lets go back to the array for an example.\n",
    "<img src=\"./files/Sorting/quicksort_pivot_2.png\" width=\"500\"/>\n",
    "i has found an element larger than the pivot and j has found an element smaller than the pivot so both have stopped. Resulting in the array follwing array:\n",
    "<img src=\"./files/Sorting/quicksort_pivot_3.png\" width=\"500\"/>\n",
    "\n",
    "Since $i \\lt j$ we swap both elements and continue. We again continue until i has found an element larger than the pivot and j has found an element smaller than the pivot. \n",
    "<img src=\"./files/Sorting/quicksort_pivot_4.png\" width=\"500\"/>\n",
    "\n",
    "Since $i \\lt j$ we swap the two resulting the following array and continue\n",
    "<img src=\"./files/Sorting/quicksort_pivot_5.png\" width=\"500\"/>\n",
    "\n",
    "i finds another element larger than the pivot which is 9. Here $i == j$ so the condition of $i \\lt j$ is still satisified. So j will search for an element smaller than the pivot which happens to be 3. At this point $i \\gt j$ so we do not swap the elements and stop the swap.\n",
    "<img src=\"./files/Sorting/quicksort_pivot_6.png\" width=\"500\"/>\n",
    "\n",
    "Now the final part of partitioning is is to swap the pivot with the element that i is pointed to. Resulting in the final array before recursively calling quicksort on the right and left sub-arrays\n",
    "<img src=\"./files/Sorting/quicksort_pivot_7.png\" width=\"500\"/>\n",
    "\n",
    "After the pivot is swapped we know that every element in position $p \\lt i$ is less than to the pivot. Also similarly every position $p \\gt i$ is greater than the pivot. The question here is what to we do with elements that are equal to the pivot. So what we do is we stop in both conditions. If i or j is at an element that is equal to the pivot they stop. This way there is no bias in either the smaller or larger sub array. This partitions the elements equal to the pivot equally among them. This way if you are sorting an array that has the same element throughout the entire array i and j will meet in the middle everytime. This essentially turns into mergesort and as we have already discussed the running time of mergesort is O(nlogn). \n",
    "\n",
    "### Running Time\n",
    "The actual recurrence relation is \n",
    "\\begin{align}\n",
    "    T(N) = T(i) + T(N-i-1) + cN\n",
    "\\end{align}\n",
    "Where i = $\\mid S_1 \\mid$ the number of elements in $S_1$ and c is the constant time for choosing a pivot. \n",
    "\n",
    "#### Worst Case\n",
    "So lets look at the worst case when we always either pick the smallest element. Well that would convert the reccurence to \n",
    "\\begin{align}\n",
    "    T(N) = T(N-1) + cN\n",
    "\\end{align}\n",
    "This is because everything is greater than the pivot resulting in the $S_2$ having N-1 elements and $\\mid S_1 \\mid$ = 0. Hence this results in decreases the problem by a constant factor result in the running time being $O(N^2)$. \n",
    "\n",
    "#### Average Case\n",
    "Now lets look at the average case. In this case we have to assume every possible size for $S_1$ is possible and equally likely. Hence each possible partition has probability $1/N$. With this assumption the average value for T(i) and for T(N-i-1) is: \n",
    "\n",
    "\\begin{align}\n",
    "    T(N) = \\frac{2}{N}[\\sum_{j=0}^{N-1} T(j)] + cN\\,\\,\\,\\,(1) \\\\\n",
    "    N*T(N) = 2[\\sum_{j=0}^{N-1} T(j)] + cN^2\\,\\,\\,\\,(2) \n",
    "\\end{align}\n",
    "\n",
    "Multiplying 1 by N resolves in equation 2. We can remove the summation by creating a telescoping with one more equation\n",
    "\n",
    "\\begin{align}\n",
    "    (N-1)*T(N-1) = 2[\\sum_{j=0}^{N-1} T(j)] + c(N-1)^2\\,\\,\\,\\,(3)\n",
    "\\end{align}\n",
    "\n",
    "Subtracting (3) from (2) results in:\n",
    "\n",
    "\\begin{align}\n",
    "    N*T(N) - (N-1)T(N-1) = 2T(N-1)+2cN-c \\\\\n",
    "    N*T(N) = (N+1)T(N-1) + 2cN\\,\\,\\,\\,(4)\n",
    "\\end{align}\n",
    "\n",
    "We drop the insignification -c from equation 4 as it's just a constant. We now have formula (4) in terms of T(N-1) only. Again the idea is to telescope but (4) is in the wrong form. So we divide (4) by $N*(N+1)$\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{T(N)}{(N+1)} = \\frac{T(N-1)}{N} + \\frac{2c}{N+1}\\,\\,\\,\\,(5)\n",
    "\\end{align}\n",
    "\n",
    "Now we can telescope\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{T(N-1)}{N} = \\frac{T(N-2)}{N-1} + \\frac{2c}{N} \\\\\n",
    "    \\frac{T(N-2)}{N-1} = \\frac{T(N-3)}{N-2} + \\frac{2c}{N-1} \\\\\n",
    "    ... \\\\\n",
    "    \\frac{T(2)}{3} = \\frac{T(1)}{2} + \\frac{2c}{3}\n",
    "\\end{align}\n",
    "\n",
    "Adding the equations together from telescoping results in \n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{T(N)}{N+1} = \\frac{T(1)}{2} + 2c* \\sum_{i=3}^{N+1} \\frac{1}{i}\\,\\,\\,\\,(6)\n",
    "\\end{align}\n",
    "\n",
    "The sum is about $log_e(N+1) + \\gamma - 3/2$, where $\\gamma \\approx 0.577$ this is known as Eulers constant, so \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{T(N)}{N+1} = O(NlogN) \\\\ \n",
    "T(N) = O(NlogN)\n",
    "\\end{align}\n",
    "\n",
    "Hence the average running time of quicksort is O(NlogN).\n",
    "\n",
    "## Bucket Sort\n",
    "Up to this point we have seen that sorting algorithms run at best O(NlogN) time. However it is possible to sort in linear time. An example of this is **bucket sort**. However for bucket sort to work the elements $A_1,A_2,...,A_n$ that we are sorting must be only positive integers. There are many variants of bucket sort but we will cover the general one:\n",
    "1. Set up initially empty buckets\n",
    "2. Iterate over the array and put each element into it's bucket\n",
    "3. Use insertion sort on each non-empty bucket\n",
    "4. Iterate over the buckets in order and put the elements back into the original array.\n",
    "\n",
    "The question here is how to chose the number of buckets. The number of buckets we chose actually affects the running time of the algorithm. Worst-case is that all the elements are put into one bucket resulting in O($N^2$). While the average case is O($N + \\frac{N^2}{k} + k$) where k is the number of buckets, we get O(N) when k=n. So it would be best to create n buckets when sorting. Another question is how to we decide which element goes into which bucket. For this we will define something called a divider. There are multiple ways to define a divider, a few examples are $\\frac{max-min}{num\\_buckets}$ and $\\frac{max}{num\\_buckets}$.<br>\n",
    "Lets look at an implementation of bucket sort:\n",
    "```python\n",
    "def bucket_sort(arr, num_buckets=None):\n",
    "    if num_buckets == None:\n",
    "        num_buckets = len(arr)\n",
    "    \n",
    "    # create our criteria for dividing into buckets\n",
    "    divider = max(arr)/num_buckets\n",
    "    \n",
    "    # create our buckets\n",
    "    buckets = [[] for _ in range(num_buckets)]\n",
    "        \n",
    "    # assign each element to a bucket\n",
    "    for x in arr:\n",
    "        b = x // size # use // to take the floor\n",
    "        \n",
    "        # if b == num_buckets then assign it to num_buckets - 1\n",
    "        if b == num_buckets:\n",
    "            buckets[num_buckets-1].append(x)\n",
    "        else:\n",
    "            buckets[b].append(x)\n",
    "   \n",
    "    # Every element now has a bucket so sort each bucket using insertion sort\n",
    "    for i in range(num_buckets):\n",
    "        buckets[i] = insertion_sort(buckets[i])\n",
    "    \n",
    "    # concatenate each bucket back into original arr\n",
    "    arr = []\n",
    "    for bucket in buckets:\n",
    "        arr.extend(bucket)\n",
    "        \n",
    "    return arr\n",
    "```\n",
    "\n",
    "\n",
    "## Radix Sort\n",
    "Radix sort also runs in O(N) time. It works well when there numbers have a large variance. Suppose we have 10 numbers from 0 - 999. For numbers like this so the idea is to use several passes of a bucket sort. The idea here is to use bucket sort on the each significant digit of the the numbers. We first will start with the least significant digit and each successive pass go onto the next until we have sorted by the most siginificant digit. Lets look at an example sorting 170, 45, 75, 90, 2, 802, 2, 66, of course all the values do not have the same number of significant digits. So if the digit is not there it is assumed to be 0, So we can look at the list as being 170, 045, 075, 090, 002, 802, 002, 066. So lets go through each pass and will assume the number of buckets is the maximum possible digit which is 9:\n",
    "\n",
    "Pass one sorting by 1's digit: <br>\n",
    "bucket 0: 170, 090<br>\n",
    "bucket 1: empty<br>\n",
    "bucket 2: 002, 802, 002<br>\n",
    "bucket 3: empty<br>\n",
    "bucket 4: empty<br>\n",
    "bucket 5: 045,075<br>\n",
    "bucket 6: 066 <br>\n",
    "bucket 7: empty<br>\n",
    "bucket 8: empty<br>\n",
    "bucket 9: empty<br>\n",
    "result: 170, 090, 002, 802, 002, 045, 075, 066<br><br>\n",
    "Pass two sorting by 10's digit:<br>\n",
    "bucket 0: 002, 802, 002<br>\n",
    "bucket 1: empty<br>\n",
    "bucket 2: empty<br>\n",
    "bucket 3: empty<br>\n",
    "bucket 4: 045<br>\n",
    "bucket 5: empty<br>\n",
    "bucket 6: 066 <br>\n",
    "bucket 7: 170,075<br>\n",
    "bucket 8: empty<br>\n",
    "bucket 9: 090<br>\n",
    "result: 002, 802, 002, 045, 066, 170, 075, 090<br><br>\n",
    "Pass two sorting by 100's digit:<br>\n",
    "bucket 0: 002, 002, 045, 066, 075, 090<br>\n",
    "bucket 1: 170<br>\n",
    "bucket 2: empty<br>\n",
    "bucket 3: empty<br>\n",
    "bucket 4: empty<br>\n",
    "bucket 5: empty<br>\n",
    "bucket 6: empty<br>\n",
    "bucket 7: empty<br>\n",
    "bucket 8: 802<br>\n",
    "bucket 9: empty<br>\n",
    "result: 002, 002, 045, 066, 075, 090, 170, 802<br><br>\n",
    "\n",
    "As we can see after sorting by the most siginificant digit the result is a sorted list. We can also go in the opposite order from most siginificant digit to least. In this case the algorithm would have to be slightly modified. First we would sort by the most siginificant digit and go through the buckets and recursively sort on buckets with more than one item by the next digit with bucket sort again and repeat this until either each bucket only has one item or we have sorted by the least siginificant digit. \n",
    "\n",
    "## Counting Sort\n",
    "Counting sort is another algorithm that runs in O(n) time. This is one of the easier sorting algorithms which requires all of the elements to be positive. We find the maximal element M in the array and create a new array of size M+1, count_arr. We then go through the unsorted array and get the counts for each element. We can then iterate through the count array to get the resulting sorted array. However the downfall to this sort is if the maximum element is large for a small list which would increase the space complexity. Following is code of how counting sort works:\n",
    "```python\n",
    "def count_sort(arr):\n",
    "    M = -1\n",
    "    # Find the max element\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] > M:\n",
    "            M = arr[i]\n",
    "            \n",
    "    # Initialize an array of size M+1\n",
    "    count_arr = [0] * (M+1)\n",
    "    \n",
    "    # Get the counts of each element\n",
    "    for i in range(len(arr)):\n",
    "        count_arr[arr[i]] += 1\n",
    "        \n",
    "    # We iterate and get the sorted list\n",
    "    # To reduce space complexity we place in back into the original unsorted array\n",
    "    i = 0\n",
    "    for j in range(M+1):\n",
    "        for k in range(count_arr[j]):\n",
    "            arr[i] = j\n",
    "            i += 1\n",
    "    \n",
    "    return arr\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
